{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> ERP Type Classification </center>\n",
    "## Binary classification of intracranial data into trial type.\n",
    "\n",
    "*Analysis by David Huberdeau*\n",
    "\n",
    "### Background\n",
    "\n",
    "Patients with epilepsy who were implanted with intracranial electrodes (depth probes and/or grids) participated in an experiment that had them reaching to different targets presented on a screen (a visuomotor association task). Trials either had no-cue, a direct cue, or a symbolic cue. Signals from the EEG record were recorded during this task, and epochs of the recordings were taken around the time that the cue was presented, as well as the time that the target was presented. Is there a difference in response in the hippocampus when a cue was symbolic compared to no-cue, or when there was a direct cue compared to no cue?\n",
    "\n",
    "### Approach\n",
    "\n",
    "In many cases, multiple electrodes recorded the hippocampal activity. Some electrodes might be better than others are detecting a difference, and the combined activity might have more information than any one alone, or than their activity averaged together.\n",
    "\n",
    "Here, I will use a decision tree [and possibly other classification and dimensionality methods] to determine if the signals differ between no-cue and the other two cue conditions, and also use it to determine which electrodes are most important for the differentiation.\n",
    "\n",
    "\n",
    "### Results\n",
    "\n",
    "A tree classifier differentiates hippocampal signals between Symbolic trials and no-cue trials (approx. 65% accuracy, where 50% is chance), while it does not differentiate Direct  trials from no-cue trials (approx. 50% accuracy). \n",
    "\n",
    "The electrode importance scores were discovered and saved to be used for further analyses.\n",
    "\n",
    "See accompanying Matlab code (`review_tree_classifier_outcomes.m`) for presentation of some of these analyses.\n",
    "\n",
    "\n",
    "\n",
    "### Conclusions\n",
    "\n",
    "Hippocampal activity significantly differed in response to the presentation of a symbolic cue, compared to the presentation of a direct cue. This is consistent with the hippocampus' role in recall of associations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load necessary software packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import os\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder as enc\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from scipy.sparse import csr_matrix, hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define some parameters:\n",
    "MAX_DEPTH = 5\n",
    "MAX_LEAF_NODES = 5\n",
    "N_ESTIMATORS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define where the data lives:\n",
    "data_dir = '/Users/david/Box Sync/projects/vma_ieeg_combined/intermediate_data'\n",
    "\n",
    "sub_data = ['LL__erp_data.mat', \n",
    "           'DO__erp_data.mat',\n",
    "           'JB__erp_data.mat',\n",
    "           'BP__erp_data.mat',\n",
    "           'VS__erp_data.mat',\n",
    "           'JK__erp_data.mat',\n",
    "           'LO__erp_data.mat',\n",
    "           'NH__erp_data.mat']\n",
    "\n",
    "labels_data = ['LL_labels.mat', \n",
    "           'DO_labels.mat',\n",
    "           'JB_labels.mat',\n",
    "           'BP_labels.mat',\n",
    "           'VS_labels.mat',\n",
    "           'JK_labels.mat',\n",
    "           'LO_labels.mat',\n",
    "           'NH_labels.mat']\n",
    "\n",
    "category_data = 'rt_set_vma1.mat'\n",
    "\n",
    "sub_channels = [\n",
    "    ['LH1', 'LH2', 'LH3', 'RH1', 'RH2', 'RH3'],\n",
    "    ['II1', 'II2', 'II3', 'HH1', 'HH2'],\n",
    "    ['SS1', 'SS2', 'II1', 'II2'],\n",
    "    ['MM1', 'MM2', 'MM3', 'MM4'],\n",
    "    ['EE1', 'FF1', 'FF2'],\n",
    "    ['HH1', 'HH2', 'HH3', 'HH4', 'HH5', 'KK1', 'KK2'],\n",
    "    ['KK1', 'KK2', 'MM1', 'MM2', 'MM3', 'OO1', 'OO2'],\n",
    "    ['DD1', 'DD2', 'DD3'],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for loading the data:\n",
    "def load_features(data_dir, sub_data):\n",
    "    factor_mat_ = scipy.io.loadmat(data_dir + os.sep + sub_data)\n",
    "    factor_mat_ = factor_mat_['features']\n",
    "    factor_mat = np.transpose(factor_mat_)\n",
    "    return factor_mat\n",
    "\n",
    "def load_labels(data_dir, labels_data):\n",
    "    labels_mat = scipy.io.loadmat(data_dir + os.sep + labels_data)\n",
    "    labels_mat_ = labels_mat['data'][0][0][0][0]\n",
    "    labels_out = []\n",
    "    for i_lab in range(len(labels_mat_)):\n",
    "        labels_out.append(labels_mat_[i_lab][0])\n",
    "    return labels_out\n",
    "\n",
    "def load_behavior(data_dir, category_data, subject_num):\n",
    "    cat_mat = scipy.io.loadmat(data_dir + os.sep + category_data)\n",
    "    cat_mat_ = cat_mat['rt_set'][0][subject_num]\n",
    "    rt = []\n",
    "    ty = []\n",
    "    for i_row in range(len(cat_mat_)):\n",
    "        rt.append(cat_mat_[i_row][0])\n",
    "        ty.append(cat_mat_[i_row][1])\n",
    "    return rt, ty\n",
    "    \n",
    "def get_channel_index(labels, channel_label):\n",
    "    for i_ch in range(len(labels)):\n",
    "        if (labels[i_ch] == channel_label):\n",
    "            k_ch = i_ch\n",
    "    return k_ch \n",
    "\n",
    "def get_channel_features(features, labels, channel_labels):\n",
    "    k_inds = []\n",
    "    for i_ch in range(len(channel_labels)):\n",
    "        k_inds.append(get_channel_index(labels, channel_labels[i_ch]))\n",
    "    sub_features_ = features[k_inds]\n",
    "    sub_features = np.transpose(sub_features_)\n",
    "    return sub_features\n",
    "\n",
    "def order_channels_by_importance(channel_list, importance_list):\n",
    "    # display the relative importance of each attribute\n",
    "    f_imp = importance_list\n",
    "    sort_order_ = np.argsort(f_imp)\n",
    "    sort_order = list(reversed(sort_order_))\n",
    "    channels_sorted = []\n",
    "    importance_sorted = []\n",
    "    for i_fact in range(0,len(sort_order)):\n",
    "        channels_sorted.append(channel_list[sort_order[i_fact]])\n",
    "        importance_sorted.append(f_imp[sort_order[i_fact]])\n",
    "    return channels_sorted, importance_sorted\n",
    "\n",
    "def save_outputs(acc_1, acc_2, ch_sort_1, ch_sort_2, score_sort_1, score_sort_2):\n",
    "    scipy.io.savemat('tree_accuracy_1.mat', {'acc_':acc_1})\n",
    "    scipy.io.savemat('tree_accuracy_2.mat', {'acc_':acc_2})\n",
    "    scipy.io.savemat('channel_sort_1.mat', {'channel_sort_':ch_sort_1})\n",
    "    scipy.io.savemat('channel_sort_2.mat', {'channel_sort_':ch_sort_2})\n",
    "    scipy.io.savemat('score_sort_1.mat', {'score_sort_':score_sort_1})\n",
    "    scipy.io.savemat('score_sort_2.mat', {'score_sort_':score_sort_2})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "LH1\n"
     ]
    }
   ],
   "source": [
    "labels = load_labels(data_dir, labels_data[0])\n",
    "k_ind = get_channel_index(labels, 'LH1')\n",
    "print(k_ind)\n",
    "print(labels[k_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_mat = scipy.io.loadmat(data_dir + os.sep + category_data)\n",
    "# cat_mat['rt_set'][0][3]\n",
    "\n",
    "rt, ty = load_behavior(data_dir, category_data, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(182, 2)\n"
     ]
    }
   ],
   "source": [
    "features = load_features(data_dir, sub_data[0])\n",
    "labels = load_labels(data_dir, labels_data[0])\n",
    "f = get_channel_features(features, labels, ['LH1', 'LH2'])\n",
    "print(np.shape(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type1 accuracy: 0.5231182795698924\n",
      "Type2 accuracy: 0.6423804226918799\n"
     ]
    }
   ],
   "source": [
    "# For each subject, load in the data, which is in matlab format (.mat)\n",
    "f_clf_acc_1 = np.empty(len(labels_data))\n",
    "f_imp_sub_1 = np.empty(len(labels_data), dtype = object)\n",
    "f_chs_sub_1 = np.empty(len(labels_data), dtype = object)\n",
    "f_clf_acc_2 = np.empty(len(labels_data))\n",
    "f_imp_sub_2 = np.empty(len(labels_data), dtype = object)\n",
    "f_chs_sub_2 = np.empty(len(labels_data), dtype = object)\n",
    "for i_sub in range(len(labels_data)):\n",
    "    feature_mat = load_features(data_dir, sub_data[i_sub])\n",
    "    labels_mat = load_labels(data_dir, labels_data[i_sub])\n",
    "    rt_mat_, ty_mat_ = load_behavior(data_dir, category_data, i_sub)\n",
    "    rt_mat = np.array(rt_mat_)\n",
    "    ty_mat = np.array(ty_mat_)\n",
    "    sub_feature = get_channel_features(feature_mat, labels_mat, sub_channels[i_sub])\n",
    "#     print(str(i_sub) + ': ' + str(np.shape(sub_feature)) + ': ' + str(np.shape(rt_mat)) + ': ' + str(np.shape(ty_mat)))\n",
    "    \n",
    "    # classify direct from no-cue:\n",
    "    ty_0_1 = [ty_mat[x] == 0 or ty_mat[x] == 1 for x in range(len(ty_mat))]\n",
    "    sfeat_0_1 = sub_feature[ty_0_1]\n",
    "    stype_0_1 = ty_mat[ty_0_1]\n",
    "    F_train, F_test, Y_train, Y_test = split(sfeat_0_1, stype_0_1, test_size = .25)\n",
    "#     model = AdaBoostClassifier(n_estimators=N_ESTIMATORS, random_state=0)\n",
    "    model = ExtraTreesClassifier(n_estimators=N_ESTIMATORS, random_state=0, max_depth=MAX_DEPTH)\n",
    "#     model = GaussianNB()\n",
    "    model.fit(F_train, Y_train)\n",
    "    # Compute the importance of each attribute\n",
    "    f_imp = model.feature_importances_\n",
    "    [chs_sorted, imp_sorted] = order_channels_by_importance(sub_channels[i_sub], f_imp)\n",
    "    f_imp_sub_1[i_sub] = imp_sorted\n",
    "    f_chs_sub_1[i_sub] = chs_sorted\n",
    "    # Compute the classification score\n",
    "    f_score = model.score(F_test, Y_test)\n",
    "    f_clf_acc_1[i_sub] = f_score\n",
    "\n",
    "    # classify symbolic from no-cue:\n",
    "    ty_0_2 = [ty_mat[x] == 0 or ty_mat[x] == 2 for x in range(len(ty_mat))]\n",
    "    sfeat_0_2 = sub_feature[ty_0_2]\n",
    "    stype_0_2 = ty_mat[ty_0_2]\n",
    "    F_train, F_test, Y_train, Y_test = split(sfeat_0_2, stype_0_2, test_size = .25)\n",
    "#     model = AdaBoostClassifier(n_estimators=N_ESTIMATORS, random_state=0)\n",
    "    model = ExtraTreesClassifier(n_estimators=N_ESTIMATORS, random_state=0, max_depth=MAX_DEPTH)\n",
    "#     model = GaussianNB()\n",
    "#     model = ComplementNB()\n",
    "    model.fit(F_train, Y_train)\n",
    "    # Compute the importance of each attribute\n",
    "    f_imp = model.feature_importances_\n",
    "    [chs_sorted, imp_sorted] = order_channels_by_importance(sub_channels[i_sub], f_imp)\n",
    "    f_imp_sub_2[i_sub] = imp_sorted\n",
    "    f_chs_sub_2[i_sub] = chs_sorted\n",
    "    # Compute the classification score\n",
    "    f_score = model.score(F_test, Y_test)\n",
    "    f_clf_acc_2[i_sub] = f_score\n",
    "    \n",
    "    \n",
    "    \n",
    "# print(f_imp_sub_1)\n",
    "# print(f_imp_sub_2)\n",
    "# print(f_chs_sub_1)\n",
    "# print(f_chs_sub_2)\n",
    "save_outputs(f_clf_acc_1, f_clf_acc_2, f_chs_sub_1, f_chs_sub_2, f_imp_sub_1, f_imp_sub_2)\n",
    "print('Type1 accuracy: ' + str(np.mean(f_clf_acc_1)))\n",
    "print('Type2 accuracy: ' + str(np.mean(f_clf_acc_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
